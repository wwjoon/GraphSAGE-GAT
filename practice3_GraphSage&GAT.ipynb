{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwjoon/GraphSAGE-GAT/blob/master/practice3_GraphSage%26GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **AAI0026 Practice 3: GraphSage and GAT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Practice 2, we constructed GNN models by using PyTorch Geometric built in GCN layer, the `GCNConv`. In this Colab we will implement the **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) and **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)) layers directly. Then we will run our models on the CORA dataset, which is a standard citation network benchmark dataset.\n",
        "\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_m9l6OYCQZP",
        "outputId": "eb88fa4f-05f5-47d1-9ab1-7a1394292846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.5MB 327kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 337kB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 15.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 16.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PRfgbfTjCRD_",
        "outputId": "70ec9c36-9bbd-4aec-a07e-7704d9bf4fc2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoXlf4MtYrbz"
      },
      "source": [
        "# 1 GNN Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQy2RBfgYut4"
      },
      "source": [
        "## Implementing Layer Modules\n",
        "\n",
        "In colab 2, we implemented a network using GCN in node and graph classification tasks. However, the GCN module we used in colab 2 is from the official library. For this problem, we will provide you with a general Graph Neural Network Stack, where you'll be able to plugin your own modules of GraphSAGE and GATs. We will use our implementations to complete node classification on CORA, which is a standard citation network benchmark dataset. In this dataset, nodes correspond to documents and edges correspond to undirected citations. Each node has a class label. The node features are elements of a bag-or-words representation of a document. For the Cora dataset, there are 2708 nodes, 5429 edges, 7 prediction classes for nodes, and 1433 features per node. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ne6Gw-CT5G"
      },
      "source": [
        "## GNN Stack Module\n",
        "\n",
        "Below is the implementation for a general GNN Module that could plugin any layers, including **GraphSage**, **GAT**, etc. This module is provided for you, and your own **GraphSage** and **GAT** layers will function as components in the GNNStack Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys8vZAFPCWWe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
        "                                    OptTensor)\n",
        "\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "# https://greeksharifa.github.io/pytorch/2021/09/04/MP/\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "        self.emb = emb\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GraphSage':\n",
        "            return GraphSage\n",
        "        elif model_type == 'GAT':\n",
        "            # When applying GAT with num heads > 1, one needs to modify the \n",
        "            # input and output dimension of the conv layers (self.convs),\n",
        "            # to ensure that the input dim of the next layer is num heads\n",
        "            # multiplied by the output dim of the previous layer.\n",
        "            # HINT: In case you want to play with multiheads, you need to change the for-loop when builds up self.convs to be\n",
        "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
        "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "          \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        if self.emb == True:\n",
        "            return x\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syDtxjxoCZgq"
      },
      "source": [
        "## GraphSage Implementation\n",
        "\n",
        "Now let's start working on our own implementation of layers! This part is to get you familiar with how to implement Pytorch layer based on Message Passing. You will be implementing the **forward**, **message** and **aggregate** functions.\n",
        "\n",
        "Generally, the **forward** function is where the actual message passing is conducted. All logic in each iteration happens in **forward**, where we'll call **propagate** function to propagate information from neighbor nodes to central nodes.  So the general paradigm will be pre-processing -> propagate -> post-processing.\n",
        "\n",
        "Recall the process of message passing we introduced. **propagate** further calls **message** which transforms information of neighbor nodes into messages, **aggregate** which aggregates all messages from neighbor nodes into one, and **update** which further generates the embedding for nodes in the next iteration.\n",
        "\n",
        "Our implementation is slightly variant from this, where we'll not explicitly implement **update**, but put the logic for updating nodes in **forward** function. To be more specific, after information is propagated, we can further conduct some operations on the output of **propagate**. The output of **forward** is exactly the embeddings after the current iteration.\n",
        "\n",
        "In addition, tensors passed to **propagate()** can be mapped to the respective nodes $i$ and $j$ by appending _i or _j to the variable name, .e.g. x_i and x_j. Note that we generally refer to $i$ as the central nodes that aggregates information, and refer to $j$ as the neighboring nodes, since this is the most common notation.\n",
        "\n",
        "Please find more details in the comments. One thing to note is that we're adding **skip connections** to our GraphSage. Formally, the update rule for our model is described as below:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
        "\\end{equation}\n",
        "\n",
        "For simplicity, we use mean aggregations where:\n",
        "\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
        "\\end{equation}\n",
        "\n",
        "Additionally, $\\ell$-2 normalization is applied after each iteration.\n",
        "\n",
        "In order to complete the work correctly, we have to understand how the different functions interact with each other. In **propagate** we can pass in any parameters we want. For example, we pass in $x$ as an parameter:\n",
        "\n",
        "... = propagate(..., $x$=($x_{central}$, $x_{neighbor}$), ...)\n",
        "\n",
        "Here $x_{central}$ and $x_{neighbor}$ represent the features from **central** nodes and from **neighbor** nodes. If we're using the same representations from central and neighbor, then $x_{central}$ and $x_{neighbor}$ could be identical.\n",
        "\n",
        "Suppose $x_{central}$ and $x_{neighbor}$ are both of shape N * d, where N is number of nodes, and d is dimension of features.\n",
        "\n",
        "Then in message function, we can take parameters called $x\\_i$ and $x\\_j$. Usually $x\\_i$ represents \"central nodes\", and $x\\_j$ represents \"neighbor nodes\". Pay attention to the shape here: $x\\_i$ and $x\\_j$ are both of shape E * d (**not N!**). $x\\_i$ is obtained by concatenating the embeddings of central nodes of all edges through lookups from $x_{central}$ we passed in propagate. Similarly, $x\\_j$ is obtained by concatenating the embeddings of neighbor nodes of all edges through lookups from $x_{neighbor}$ we passed in propagate.\n",
        "\n",
        "Let's look at an example. Suppose we have 4 nodes, so $x_{central}$ and $x_{neighbor}$ are of shape 4 * d. We have two edges (1, 2) and (3, 0). Thus, $x\\_i$ is obtained by $[x_{central}[1]^T; x_{central}[3]^T]^T$, and $x\\_j$ is obtained by $[x_{neighbor}[2]^T; x_{neighbor}[0]^T]^T$\n",
        "\n",
        "<font color='red'>For the following questions, DON'T refer to any existing implementations online.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwG4HqCFCaOD"
      },
      "outputs": [],
      "source": [
        "class GraphSage(MessagePassing):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, normalize = True,\n",
        "                 bias = False, **kwargs):  \n",
        "        super(GraphSage, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message and update functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embedding \n",
        "        #            for central node.\n",
        "        # self.lin_r is the linear transformation that you apply to aggregated \n",
        "        #            message from neighbors.\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        self.lin_r.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any post-processing (our update rule).\n",
        "        # 1. First call propagate function to conduct the message passing.\n",
        "        #    1.1 See there for more information: \n",
        "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        #    1.2 We use the same representations for central (x_central) and \n",
        "        #        neighbor (x_neighbor) nodes, which means you'll pass x=(x, x) \n",
        "        #        to propagate.\n",
        "        # 2. Update our node embedding with skip connection.\n",
        "        # 3. If normalize is set, do L-2 normalization (defined in \n",
        "        #    torch.nn.functional)\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function here.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        # The axis along which to index number of nodes.\n",
        "        node_dim = self.node_dim\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: \n",
        "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qx1bA2m1SWA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcfF3RACdLD"
      },
      "source": [
        "## GAT Implementation\n",
        "\n",
        "Attention mechanisms have become the state-of-the-art in many sequence-based tasks such as machine translation and learning sentence representations. One of the major benefits of attention-based mechanisms is their ability to focus on the most relevant parts of the input to make decisions. In this problem, we will see how attention mechanisms can be used to perform node classification of graph-structured data through the usage of Graph Attention Networks (GATs).\n",
        "\n",
        "The building block of the Graph Attention Network is the graph attention layer, which is a variant of the aggregation function . Let $N$ be the number of nodes and $F$ be the dimension of the feature vector for each node. The input to each graph attentional layer is a set of node features: $\\mathbf{h} = \\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}$\\}, $\\overrightarrow{h_i} \\in R^F$. The output of each graph attentional layer is a new set of node features, which may have a new dimension $F'$: $\\mathbf{h'} = \\{\\overrightarrow{h_1'}, \\overrightarrow{h_2'}, \\dots, \\overrightarrow{h_N'}\\}$, with $\\overrightarrow{h_i'} \\in \\mathbb{R}^{F'}$.\n",
        "\n",
        "We will now describe this transformation of the input features into higher-level features performed by each graph attention layer. First, a shared linear transformation parametrized by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ is applied to every node. Next, we perform self-attention on the nodes. We use a shared attentional mechanism:\n",
        "\\begin{equation} \n",
        "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
        "\\end{equation}\n",
        "\n",
        "This mechanism computes the attention coefficients that capture the importance of node $j$'s features to node $i$:\n",
        "\\begin{equation}\n",
        "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
        "\\end{equation}\n",
        "The most general formulation of self-attention allows every node to attend to all other nodes which drops all structural information. To utilize graph structure in the attention mechanisms, we can use masked attention. In masked attention, we only compute $e_{ij}$ for nodes $j \\in \\mathcal{N}_i$ where $\\mathcal{N}_i$ is some neighborhood of node $i$ in the graph.\n",
        "\n",
        "To easily compare coefficients across different nodes, we normalize the coefficients across $j$ using a softmax function:\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "\\end{equation}\n",
        "\n",
        "For this problem, our attention mechanism $a$ will be a single-layer feedforward neural network parametrized by a weight vector $\\overrightarrow{a} \\in \\mathbb{R}^{F'}$, followed by a LeakyReLU nonlinearity (with negative input slope 0.2). Let $\\cdot^T$ represent transposition and $||$ represent concatenation. The coefficients computed by our attention mechanism may be expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
        "\\end{equation}\n",
        "\n",
        "For the following questions, we denote $\\alpha_l = [...,\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i},...]$ and $\\alpha_r = [..., \\overrightarrow{a_r}^T \\mathbf{W_r} \\overrightarrow{h_j}, ...]$.\n",
        "\n",
        "\n",
        "At every layer of GAT, after the attention coefficients are computed for that layer, the aggregation function can be computed by a weighted sum of neighborhood messages, where weights are specified by $\\alpha_{ij}$.\n",
        "\n",
        "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
        "\n",
        "\\begin{equation}\n",
        "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
        "\\end{equation}\n",
        "\n",
        "To stabilize the learning process of self-attention, we use multi-head attention. To do this we use $K$ independent attention mechanisms, or ``heads'' compute output features as in the above equations. Then, we concatenate these output feature representations:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $||$ is concentation, $\\alpha_{ij}^{(k)}$ are the normalized attention coefficients computed by the $k$-th attention mechanism $(a^k)$, and $\\mathbf{W}^{(k)}$ is the corresponding input linear transformation's weight matrix. Note that for this setting, $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4j45gTpCeXO"
      },
      "outputs": [],
      "source": [
        "class GAT(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, heads = 2,\n",
        "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "        self.att_l = None\n",
        "        self.att_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embeddings \n",
        "        # BEFORE message passing.\n",
        "        # Pay attention to dimensions of the linear layers, since we're using \n",
        "        # multi-head attention.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.lin_r = self.lin_l\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
        "        # You have to deal with multi-head scenarios.\n",
        "        # Use nn.Parameter instead of nn.Linear\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
        "        # 1. First apply linear transformation to node embeddings, and split that \n",
        "        #    into multiple heads. We use the same representations for source and\n",
        "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
        "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
        "        # 3. Call propagate function to conduct the message passing. \n",
        "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
        "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        # 4. Transform the output back to the shape of N * d.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        x_l = self.lin_l(x).reshape(-1, H, C)\n",
        "        x_r = self.lin_r(x).reshape(-1, H, C)\n",
        "        alpha_l = self.att_l * x_l\n",
        "        alpha_r = self.att_r * x_r\n",
        "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)\n",
        "        out = out.reshape(-1, H*C)\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function. Putting the attention in message \n",
        "        # instead of in update is a little tricky.\n",
        "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
        "        #    and apply leaky Relu.\n",
        "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
        "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
        "        # 3. Apply dropout to attention weights (alpha).\n",
        "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
        "        #    should be of shape E * H * d.\n",
        "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
        "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
        "        # Don't worry if you deviate from this.\n",
        "\n",
        "        alpha = F.leaky_relu(alpha_i + alpha_j, negative_slope=self.negative_slope)\n",
        "        if ptr:\n",
        "            att_weight = F.softmax(alpha, ptr)\n",
        "        else:\n",
        "            att_weight = torch_geometric.utils.softmax(alpha, index)\n",
        "        # Fill below with 2 lines \n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
        "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        \n",
        "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='sum')\n",
        "        ############################################################################\n",
        "    \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2dkgSuWCheU"
      },
      "source": [
        "## Building Optimizers\n",
        "\n",
        "This function has been implemented for you. **For grading purposes please use the default Adam optimizer**, but feel free to play with other types of optimizers on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_TIQ8NPCjBP"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYdWFwYCkwY"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Here we provide you with the functions to train and test. **Please do not modify this part for grading purposes.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tZMWRc8CmGg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train(dataset, args):\n",
        "    \n",
        "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
        "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, \n",
        "                            args)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    losses = []\n",
        "    test_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            pred = pred[batch.train_mask]\n",
        "            label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          test_acc = test(test_loader, model)\n",
        "          test_accs.append(test_acc)\n",
        "          print(\"Epoch \", epoch, \"Loss: \", total_loss, \"Test Acc.: \", test_acc)\n",
        "        else:\n",
        "          test_accs.append(test_accs[-1])\n",
        "    return test_accs, losses\n",
        "\n",
        "def test(loader, model, is_validation=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        mask = data.val_mask if is_validation else data.test_mask\n",
        "        # node classification: only evaluate on nodes in test set\n",
        "        pred = pred[mask]\n",
        "        label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    total = 0\n",
        "    for data in loader.dataset:\n",
        "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "    return correct / total\n",
        "  \n",
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-h7jIsCns4"
      },
      "source": [
        "## Let's Start the Training!\n",
        "\n",
        "We will be working on the CORA dataset on node-level classification.\n",
        "\n",
        "This part is implemented for you. **For grading purposes, please do not modify the default parameters.** However, feel free to play with different configurations just for fun!\n",
        "\n",
        "**Submit your best accuracy and loss on Gradescope.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qe9B45l9Cpz2",
        "outputId": "134df793-e539-4f23-b3be-e5a095817ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node task. test set size: 140\n",
            "Epoch  0 Loss:  1.9639238119125366 Test Acc.:  0.066\n",
            "Epoch  10 Loss:  1.594685673713684 Test Acc.:  0.458\n",
            "Epoch  20 Loss:  1.0178499221801758 Test Acc.:  0.576\n",
            "Epoch  30 Loss:  0.5653009414672852 Test Acc.:  0.65\n",
            "Epoch  40 Loss:  0.3797193765640259 Test Acc.:  0.692\n",
            "Epoch  50 Loss:  0.24454887211322784 Test Acc.:  0.69\n",
            "Epoch  60 Loss:  0.2770047187805176 Test Acc.:  0.706\n",
            "Epoch  70 Loss:  0.23730751872062683 Test Acc.:  0.666\n",
            "Epoch  80 Loss:  0.26764926314353943 Test Acc.:  0.714\n",
            "Epoch  90 Loss:  0.22666846215724945 Test Acc.:  0.706\n",
            "Epoch  100 Loss:  0.2219611406326294 Test Acc.:  0.662\n",
            "Epoch  110 Loss:  0.2490205466747284 Test Acc.:  0.7\n",
            "Epoch  120 Loss:  0.20205456018447876 Test Acc.:  0.698\n",
            "Epoch  130 Loss:  0.17690697312355042 Test Acc.:  0.682\n",
            "Epoch  140 Loss:  0.19905562698841095 Test Acc.:  0.692\n",
            "Epoch  150 Loss:  0.19539164006710052 Test Acc.:  0.684\n",
            "Epoch  160 Loss:  0.2004736065864563 Test Acc.:  0.696\n",
            "Epoch  170 Loss:  0.22288809716701508 Test Acc.:  0.706\n",
            "Epoch  180 Loss:  0.18927131593227386 Test Acc.:  0.732\n",
            "Epoch  190 Loss:  0.23453034460544586 Test Acc.:  0.664\n",
            "Epoch  200 Loss:  0.18244755268096924 Test Acc.:  0.706\n",
            "Epoch  210 Loss:  0.1992729753255844 Test Acc.:  0.684\n",
            "Epoch  220 Loss:  0.1829512119293213 Test Acc.:  0.708\n",
            "Epoch  230 Loss:  0.18747372925281525 Test Acc.:  0.712\n",
            "Epoch  240 Loss:  0.20102237164974213 Test Acc.:  0.724\n",
            "Epoch  250 Loss:  0.1509690284729004 Test Acc.:  0.692\n",
            "Epoch  260 Loss:  0.15748845040798187 Test Acc.:  0.704\n",
            "Epoch  270 Loss:  0.14114339649677277 Test Acc.:  0.708\n",
            "Epoch  280 Loss:  0.15719254314899445 Test Acc.:  0.684\n",
            "Epoch  290 Loss:  0.13716787099838257 Test Acc.:  0.676\n",
            "Epoch  300 Loss:  0.1504090130329132 Test Acc.:  0.7\n",
            "Epoch  310 Loss:  0.1730610728263855 Test Acc.:  0.654\n",
            "Epoch  320 Loss:  0.17946171760559082 Test Acc.:  0.7\n",
            "Epoch  330 Loss:  0.2160581797361374 Test Acc.:  0.712\n",
            "Epoch  340 Loss:  0.2033557891845703 Test Acc.:  0.672\n",
            "Epoch  350 Loss:  0.13133187592029572 Test Acc.:  0.708\n",
            "Epoch  360 Loss:  0.1487676352262497 Test Acc.:  0.694\n",
            "Epoch  370 Loss:  0.20359817147254944 Test Acc.:  0.708\n",
            "Epoch  380 Loss:  0.18292388319969177 Test Acc.:  0.702\n",
            "Epoch  390 Loss:  0.26059070229530334 Test Acc.:  0.688\n",
            "Epoch  400 Loss:  0.165866419672966 Test Acc.:  0.71\n",
            "Epoch  410 Loss:  0.1764431893825531 Test Acc.:  0.694\n",
            "Epoch  420 Loss:  0.13906536996364594 Test Acc.:  0.69\n",
            "Epoch  430 Loss:  0.1936257779598236 Test Acc.:  0.7\n",
            "Epoch  440 Loss:  0.11591929197311401 Test Acc.:  0.712\n",
            "Epoch  450 Loss:  0.19589608907699585 Test Acc.:  0.702\n",
            "Epoch  460 Loss:  0.17475824058055878 Test Acc.:  0.696\n",
            "Epoch  470 Loss:  0.12218017876148224 Test Acc.:  0.694\n",
            "Epoch  480 Loss:  0.21204736828804016 Test Acc.:  0.688\n",
            "Epoch  490 Loss:  0.17230136692523956 Test Acc.:  0.704\n",
            "Maximum accuracy: 0.732\n",
            "Minimum loss: 0.10110029578208923\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GraphSage\n",
        "\n",
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
        "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, \n",
        "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
        "         'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GraphSage']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\") \n",
        "            test_accs, losses = train(dataset, args) \n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1bYmz-q93HLk",
        "outputId": "b6196ba1-ae8d-4537-a033-51854bc39d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node task. test set size: 140\n",
            "Epoch  0 Loss:  1.9566068649291992 Test Acc.:  0.104\n",
            "Epoch  10 Loss:  1.0250568389892578 Test Acc.:  0.486\n",
            "Epoch  20 Loss:  0.2713371515274048 Test Acc.:  0.674\n",
            "Epoch  30 Loss:  0.13003231585025787 Test Acc.:  0.722\n",
            "Epoch  40 Loss:  0.09789977967739105 Test Acc.:  0.712\n",
            "Epoch  50 Loss:  0.11593309044837952 Test Acc.:  0.678\n",
            "Epoch  60 Loss:  0.09242720901966095 Test Acc.:  0.666\n",
            "Epoch  70 Loss:  0.12858586013317108 Test Acc.:  0.722\n",
            "Epoch  80 Loss:  0.04588910937309265 Test Acc.:  0.726\n",
            "Epoch  90 Loss:  0.09208666533231735 Test Acc.:  0.718\n",
            "Epoch  100 Loss:  0.05639316514134407 Test Acc.:  0.726\n",
            "Epoch  110 Loss:  0.08017326891422272 Test Acc.:  0.71\n",
            "Epoch  120 Loss:  0.09598377346992493 Test Acc.:  0.732\n",
            "Epoch  130 Loss:  0.050167299807071686 Test Acc.:  0.704\n",
            "Epoch  140 Loss:  0.04777543619275093 Test Acc.:  0.734\n",
            "Epoch  150 Loss:  0.05792825669050217 Test Acc.:  0.72\n",
            "Epoch  160 Loss:  0.05302969738841057 Test Acc.:  0.728\n",
            "Epoch  170 Loss:  0.07059156149625778 Test Acc.:  0.716\n",
            "Epoch  180 Loss:  0.07883428782224655 Test Acc.:  0.69\n",
            "Epoch  190 Loss:  0.03828642889857292 Test Acc.:  0.722\n",
            "Epoch  200 Loss:  0.05661905184388161 Test Acc.:  0.708\n",
            "Epoch  210 Loss:  0.06892883777618408 Test Acc.:  0.734\n",
            "Epoch  220 Loss:  0.10103548318147659 Test Acc.:  0.744\n",
            "Epoch  230 Loss:  0.041787538677453995 Test Acc.:  0.724\n",
            "Epoch  240 Loss:  0.04035891592502594 Test Acc.:  0.748\n",
            "Epoch  250 Loss:  0.1070757731795311 Test Acc.:  0.732\n",
            "Epoch  260 Loss:  0.06245577707886696 Test Acc.:  0.686\n",
            "Epoch  270 Loss:  0.08435805141925812 Test Acc.:  0.718\n",
            "Epoch  280 Loss:  0.04722487926483154 Test Acc.:  0.714\n",
            "Epoch  290 Loss:  0.06513091176748276 Test Acc.:  0.714\n",
            "Epoch  300 Loss:  0.050673872232437134 Test Acc.:  0.706\n",
            "Epoch  310 Loss:  0.05738869309425354 Test Acc.:  0.726\n",
            "Epoch  320 Loss:  0.05891233682632446 Test Acc.:  0.73\n",
            "Epoch  330 Loss:  0.08084598928689957 Test Acc.:  0.726\n",
            "Epoch  340 Loss:  0.05252746492624283 Test Acc.:  0.684\n",
            "Epoch  350 Loss:  0.041383046656847 Test Acc.:  0.704\n",
            "Epoch  360 Loss:  0.11209285259246826 Test Acc.:  0.71\n",
            "Epoch  370 Loss:  0.037695128470659256 Test Acc.:  0.744\n",
            "Epoch  380 Loss:  0.07577959448099136 Test Acc.:  0.72\n",
            "Epoch  390 Loss:  0.051607899367809296 Test Acc.:  0.71\n",
            "Epoch  400 Loss:  0.045833077281713486 Test Acc.:  0.728\n",
            "Epoch  410 Loss:  0.06033756956458092 Test Acc.:  0.718\n",
            "Epoch  420 Loss:  0.0716925784945488 Test Acc.:  0.708\n",
            "Epoch  430 Loss:  0.06118988245725632 Test Acc.:  0.72\n",
            "Epoch  440 Loss:  0.0878349095582962 Test Acc.:  0.7\n",
            "Epoch  450 Loss:  0.06317002326250076 Test Acc.:  0.706\n",
            "Epoch  460 Loss:  0.06569656729698181 Test Acc.:  0.712\n",
            "Epoch  470 Loss:  0.05509461835026741 Test Acc.:  0.72\n",
            "Epoch  480 Loss:  0.08882367610931396 Test Acc.:  0.744\n",
            "Epoch  490 Loss:  0.05244319140911102 Test Acc.:  0.716\n",
            "Maximum accuracy: 0.748\n",
            "Minimum loss: 0.02610740251839161\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GAT\n",
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
        "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, \n",
        "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
        "         'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GAT']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\") \n",
        "            test_accs, losses = train(dataset, args) \n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHELqjARZ1W5"
      },
      "source": [
        "## Question 1.1: What is the maximum accuracy you could get on test set for GraphSage?\n",
        "\n",
        "Maximum accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlCtBEBLMBkR"
      },
      "source": [
        "## Question 1.2: What is the maximum accuracy you could get on test set for GAT?\n",
        "\n",
        "Maximum accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "In order to get credit, you need to submit the `ipynb` file of Colab 3 to LMS.\n",
        "To get this file, click `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}